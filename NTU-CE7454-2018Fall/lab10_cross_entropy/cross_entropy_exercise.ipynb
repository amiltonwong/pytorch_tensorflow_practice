{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Cross-entropy loss -- exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Cross Entropy Criterion and call it criterion. The command is nn.CrossEntropyLoss()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion= # COMPLETE HERE\n",
    "\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose that there only two classes (class 0 and class 1).\n",
    "### Suppose we have a batch of three data points: \n",
    "### ${\\bf x^{(0)}}$ belongs to class 0\n",
    "### ${\\bf x^{(1)}}$belongs to class 1\n",
    "### ${\\bf x^{(2)}}$ belongs to class 1\n",
    "### Put the labels of each of these point a LongTensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.LongTensor( # COMPLETE HERE   )\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a batch of scores: each row corresponds to the scores associated with a data point. So your batch of scores should look likes something like:\n",
    "\n",
    "$$\n",
    "\\text{scores} \\;\\; = \\;\\; \\begin{bmatrix}\n",
    "s_0^{(0)} & s_1^{(0)} & \\\\\n",
    "s_0^{(1)} & s_1^{(1)} & \\\\\n",
    "s_0^{(2)} & s_1^{(2)} & \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### You will need to create a tensor of the form torch.Tensor( [ [ ], [ ], [ ] ] ). Don't forget the extra square brackets!\n",
    "\n",
    "### Choose scores that will leads to a loss very close to zero, let say around or smaller than 0.05 (indicating that the scores are very good with respect to the labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = # COMPLETE HERE\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display your batch of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.display_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the criterion to compute the average loss on this batch -- it needs to be around or smaller than 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss = # COMPLETE HERE\n",
    "\n",
    "print(average_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
