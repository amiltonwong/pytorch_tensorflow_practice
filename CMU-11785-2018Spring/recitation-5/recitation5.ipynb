{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks Tutorial\n",
    "\n",
    "In HW1 you've worked on building simple MLP models that you used to classify single inputs of MNIST images into ten classes, and in HW2 you will work with CNNs to recognize objects in images. However, real world data are rarely standalone fixed-size vectors that do not depend on context - speech segments, for instance, have arbitrary lengths, and evidently depend on context. In this tutorial, we will take a small step in this direction by building a basic RNN for generating text by learning from some sample text files that we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You must random walk before you rnn\n",
    "\n",
    "Let's begin by loading a choice selection of tweets by the leader of the free world, courtesy a dataset available on Kaggle (https://www.kaggle.com/kingburrito666/better-donald-trump-tweets), formatted for this recitation*.\n",
    "\n",
    "\\**11785 staff are not responsible for content, accuracy, or sanity of this dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "history_length = 3\n",
    "\n",
    "def process_input(filename):\n",
    "    rawtext = open(filename).read()\n",
    "    sequences = [rawtext[i:i+history_length] for i in range(int(len(rawtext)/history_length))]\n",
    "    stats = Counter(sequences)\n",
    "    tokens = []\n",
    "    counts = []\n",
    "    for i in stats.most_common():\n",
    "        tokens.append(i[0])\n",
    "        counts.append(i[1])\n",
    "    return stats\n",
    "    \n",
    "def next_char(cur,stats):\n",
    "    seed = cur[1:]\n",
    "    candidates = []\n",
    "    candidatec = []\n",
    "    for k in stats.keys():\n",
    "        if seed==k[:-1]:\n",
    "            candidates.append(k)\n",
    "            candidatec.append(float(stats[k]))\n",
    "    candidatep = [x/sum(candidatec) for x in candidatec]\n",
    "    return candidates[np.random.choice(len(candidatec),p=candidatep)]\n",
    "\n",
    "def sample(length,running_state, stats):\n",
    "    output = ''\n",
    "    for i in range(length):\n",
    "        output+=running_state[0]\n",
    "        running_state = next_char(running_state,stats)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oban an t co bad firs  the  speco wor busins han h'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_stats = process_input('tweets.txt')\n",
    "sample(50,'oba',tweet_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince you that this is a reasonable approach, let's take a more reasonable example of the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the min to mings fore's to sand be: the them? thei\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_stats = process_input('hamlet.txt')\n",
    "sample(50, 'the',hamlet_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we further increase the running history that this extremely simple model keeps, we see better and better examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and  vegallowers   new https     your see   t come\n",
      "the that fles, and by a sea of outrageousand them?\n"
     ]
    }
   ],
   "source": [
    "history_length = 4\n",
    "tweet_stats = process_input('tweets.txt')\n",
    "print(sample(50,'and ',tweet_stats))\n",
    "hamlet_stats = process_input('hamlet.txt')\n",
    "print(sample(50,'the ',hamlet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obamas belief effortunited past the field  carolin\n",
      "to be, or not to say we end the question: whether \n"
     ]
    }
   ],
   "source": [
    "history_length = 5\n",
    "tweet_stats = process_input('tweets.txt')\n",
    "print(sample(50,'obama',tweet_stats))\n",
    "hamlet_stats = process_input('hamlet.txt')\n",
    "print(sample(50,'to be',hamlet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some inherent limitations to this approach. Character-level n-grams do get better with increasing n, but we run into all sorts of limitations in terms of memory (possible combinations are exponential in the size of alphabet) and sparsity (it is HIGHLY unlikely that you will observe all possible n-grams in the data). There exists a lot of literature over the past few decades dealing with engineering NLP techniques to get around this (cf. [1]), but even then we are limited by the fact that information from (n+1) steps ago gets washed out.\n",
    "\n",
    "\n",
    "[1] Chen and Goodman. An Empirical Study of Smoothing Techniques for Language Modeling. 1998."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "So far, we've been working with networks where the input size was fixed. Today let's take the example of English language. A typical method of vectorizing words is to convert them into a one-hot representation over the vocabulary.\n",
    "\n",
    "Let's take one such sample vector representation x which belongs to the set of all such vectors X (the vocabulary).\n",
    "\n",
    "In the previous section, we implemented a Markov chain that generates sentences given a seed and statistics over the training data. From a high-level standpoint, we were looking at the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef markov_generator(x):\\n    current_state = current_state\\n    for i in range(some target length):\\n        output += generate_single_step_output(current_state)\\n        new_state = random_process(current_state)\\n        current_state = new_state\\n    return output\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is meant as an overview, not meant to compile\n",
    "'''\n",
    "def markov_generator(x):\n",
    "    current_state = current_state\n",
    "    for i in range(some target length):\n",
    "        output += generate_single_step_output(current_state)\n",
    "        new_state = random_process(current_state)\n",
    "        current_state = new_state\n",
    "    return output\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a string of length n corresponds to the current state. Evidently, forgetting your previous state at every step is not going to be able to capture long term dependencies. We have to stop being Markovian!\n",
    "\n",
    "Instead, what we need is a method of *storing* what we have observed so far. Consider the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass generator_with_memory:\\n    def __init__():\\n        self.memory_state = self.init_memory()\\n    def step(x):\\n        self.memory_state = smart_process(x,self.memory_state)\\n        y = generate_output(self.memory_state)\\n        return y\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is meant as an overview, not meant to compile\n",
    "'''\n",
    "class generator_with_memory:\n",
    "    def __init__():\n",
    "        self.memory_state = self.init_memory()\n",
    "    def step(x):\n",
    "        self.memory_state = smart_process(x,self.memory_state)\n",
    "        y = generate_output(self.memory_state)\n",
    "        return y\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining an object with an associated memory_state and a smart choice of smart_process() and generate_output(), we might be able to capture long term dependencies by *storing* them in our memory_state.\n",
    "\n",
    "How can we create such functions? One choice could be to have a linear transform between all associated vectors. Let's look at our vectors so far. Let's call memory_state as h for convenience.\n",
    "\n",
    "x: Input vector\n",
    "\n",
    "y: Output vector\n",
    "\n",
    "h: \"Hidden\" memory_state vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef step(x):\\n    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\\n    y = np.dot(self.W_hy, self.h)\\n    return y\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is meant as an overview, not meant to compile\n",
    "'''\n",
    "def step(x):\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update step incorporates both the current input x and the current memory state h. This can replace smart_process above and incorporate memory into our model! We can initialize the model parameters W_hh, W_xh, W_hy randomly and learn them from our data.\n",
    "\n",
    "It is also important to note that this method is able to work with arbitrary length sequences. We can just step through the entire sequence and rely on h to keep track of long range dependencies. Thus we have moved away from the fixed input size paradigm as well! You can visualize the process of stepping through the sequence from the following image (credits: Wikipedia)\n",
    "\n",
    "<img src=\"rnn.svg\" width=700>\n",
    "\n",
    "You can come up with more imaginative ways to step through the sequence. For instance, why should we restrict ourselves to a flat sequence? A step combines the current input x_t with the current hidden state h_t. One can recursively apply this combination step, such as the authors of [2] who used this idea for sentiment analysis. \n",
    "\n",
    "\n",
    "<img src=\"recursive1.png\" width=400>\n",
    "\n",
    "One can also stack things - instead of having just one recurrent layer you can use\n",
    "\n",
    "y1 = rnn1.step(x)\n",
    "\n",
    "y2 = rnn2.step(y1)\n",
    "\n",
    "and so on.\n",
    "\n",
    "[2] Socher, Manning, Ng. Recursive deep models for semantic compositionality over a sentiment treebank. EMNLP 2013.\n",
    "\n",
    "\n",
    "# RNNs using PyTorch\n",
    "\n",
    "Let's work with an example task and implement an RNN using PyTorch. Specifically, let's look at a simple time series prediction problem. For the purpose of this recitation we will work on a very simple time series prediction problem adapted from PyTorch examples [3]. We'll generate a random set of sinusoidal waves and use LSTMCell units to learn these waves and predict the next few steps.\n",
    "\n",
    "\n",
    "[3] https://github.com/pytorch/examples/tree/master/time_sequence_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "T = 80    # range from which ints are sampled\n",
    "L = 1000  # Length of generated sequence\n",
    "N = 100   # number of examples\n",
    "future = 1000 # length of sequence to predict\n",
    "# generating a sinusoidal time series\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(- 4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(x / 1.0 / T).astype('float64')\n",
    "\n",
    "\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 32)\n",
    "        self.lstm2 = nn.LSTMCell(32, 32)\n",
    "        self.linear = nn.Linear(32, 1)\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        c_t = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        h_t2 = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        c_t2 = Variable(torch.zeros(input.size(0), 32).double(), requires_grad=False)\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# predicting future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n",
    "    \n",
    "def save_plot_wave(y_gen):\n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.title('Predict future values for time sequence', fontsize=30)\n",
    "    plt.xlabel('x', fontsize=20)\n",
    "    plt.ylabel('y', fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.plot(np.arange(input.size(1)), y_gen[0][:input.size(1)], 'b', linewidth = 2.0)\n",
    "    plt.plot(np.arange(input.size(1), input.size(1) + future), y_gen[0][input.size(1):], 'b' + ':', linewidth = 2.0)\n",
    "    plt.savefig('predict%d.pdf'%i)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the staggered nature of input and target data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Variable(torch.from_numpy(data[1:, :-1]), requires_grad=False)\n",
    "target = Variable(torch.from_numpy(data[1:, 1:]), requires_grad=False)\n",
    "test_input = Variable(torch.from_numpy(data[:1, :-1]), requires_grad=False)\n",
    "test_target = Variable(torch.from_numpy(data[:1, 1:]), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the LBFGS optimizer. It is used for parameter estimation by minimizing a smooth f(x) over unconstrained real-valued vector x. It needs to reevaluate the function multiple times, so you have to pass in a closure that allows recomputation. The closure should clear the gradients, compute the loss, and return it [4]. LBFGS also does not support per-parameter options.\n",
    "\n",
    "\n",
    "[4] http://pytorch.org/docs/master/optim.html#optimizer-step-closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Loss: 0.534735817744\n",
      "Loss: 0.51595643509\n",
      "Loss: 0.487655250214\n",
      "Loss: 0.476866620552\n",
      "Loss: 0.443208983365\n",
      "Loss: 0.354411918037\n",
      "Loss: 0.230974896977\n",
      "Loss: 6.21625536659\n",
      "Loss: 0.107385785838\n",
      "Loss: 0.0244810777848\n",
      "Loss: 0.00949078034371\n",
      "Loss: 0.00899265775527\n",
      "Loss: 0.00716001025314\n",
      "Loss: 0.00574255749538\n",
      "Loss: 0.00422940005235\n",
      "Loss: 0.00372545641453\n",
      "Loss: 0.00351897569446\n",
      "Loss: 0.00316297494398\n",
      "Loss: 0.00197494376456\n",
      "Loss: 0.0012997484383\n",
      "Test loss: 0.00116922468676\n",
      "Step:  1\n",
      "Loss: 0.00145462772409\n",
      "Loss: 0.00118182828882\n",
      "Loss: 0.00114993069323\n",
      "Loss: 0.00111182542918\n",
      "Loss: 0.00104719865128\n",
      "Loss: 0.000952630861115\n",
      "Loss: 0.000879430665862\n",
      "Loss: 0.000858234694016\n",
      "Loss: 0.000823140827356\n",
      "Loss: 0.000815607769945\n",
      "Loss: 0.000785582041389\n",
      "Loss: 0.000768247239385\n",
      "Loss: 0.000739432594294\n",
      "Loss: 0.000691610451779\n",
      "Loss: 0.000639641483622\n",
      "Loss: 0.00056794222483\n",
      "Loss: 0.000582874034494\n",
      "Loss: 0.000521702656905\n",
      "Loss: 0.000527696125628\n",
      "Loss: 0.000490990391194\n",
      "Test loss: 0.000364178401968\n",
      "Step:  2\n",
      "Loss: 0.000475000223186\n",
      "Loss: 0.000443743208666\n",
      "Loss: 0.000405212550299\n",
      "Loss: 0.000363450478623\n",
      "Loss: 0.000349381608751\n",
      "Loss: 0.000329962760153\n",
      "Loss: 0.00032021876075\n",
      "Loss: 0.00031586417019\n",
      "Loss: 0.000313784756038\n",
      "Loss: 0.00031245535365\n",
      "Loss: 0.000311075650661\n",
      "Loss: 0.000307976767579\n",
      "Loss: 0.000301441406567\n",
      "Loss: 0.000287378695643\n",
      "Loss: 0.000260281280745\n",
      "Loss: 0.000258994054642\n",
      "Loss: 0.000232215869055\n",
      "Loss: 0.000223077141369\n",
      "Loss: 0.000210962214309\n",
      "Loss: 0.000203371911198\n",
      "Test loss: 0.000125290996629\n",
      "Step:  3\n",
      "Loss: 0.000198622762651\n",
      "Loss: 0.000193427880996\n",
      "Loss: 0.000189139375684\n",
      "Loss: 0.000184419686344\n",
      "Loss: 0.000179143510437\n",
      "Loss: 0.000170780254305\n",
      "Loss: 0.000179680505847\n",
      "Loss: 0.000164723393531\n",
      "Loss: 0.00016305363411\n",
      "Loss: 0.000160433839687\n",
      "Loss: 0.000158777652822\n",
      "Loss: 0.000157648103219\n",
      "Loss: 0.000156508232341\n",
      "Loss: 0.000155578153118\n",
      "Loss: 0.000150368320592\n",
      "Loss: 0.000141645111445\n",
      "Loss: 0.000129853579478\n",
      "Loss: 0.000146714386213\n",
      "Loss: 0.000117126864642\n",
      "Loss: 0.000114706457878\n",
      "Test loss: 7.45965447817e-05\n",
      "Step:  4\n",
      "Loss: 0.000111926204751\n",
      "Loss: 0.000108899712335\n",
      "Loss: 0.000107294608565\n",
      "Loss: 0.000106671826036\n",
      "Loss: 0.000106522464644\n",
      "Loss: 0.000106427519517\n",
      "Loss: 0.000106135408676\n",
      "Loss: 0.000105073525117\n",
      "Loss: 0.000104012128891\n",
      "Loss: 0.000103348722002\n",
      "Loss: 0.000102839505304\n",
      "Loss: 0.000102803400478\n",
      "Loss: 0.000102594312476\n",
      "Loss: 0.000102549012321\n",
      "Loss: 0.000102485443702\n",
      "Loss: 0.000102429646339\n",
      "Loss: 0.000102082526553\n",
      "Loss: 0.00010038107925\n",
      "Loss: 9.70681808915e-05\n",
      "Loss: 9.29469168545e-05\n",
      "Test loss: 3.742124371e-05\n",
      "Step:  5\n",
      "Loss: 8.55601804036e-05\n",
      "Loss: 7.54651407203e-05\n",
      "Loss: 7.23616353648e-05\n",
      "Loss: 7.19238419828e-05\n",
      "Loss: 6.96617404737e-05\n",
      "Loss: 6.84061907269e-05\n",
      "Loss: 6.55017273389e-05\n",
      "Loss: 6.21217438185e-05\n",
      "Loss: 6.0200384921e-05\n",
      "Loss: 5.94776952751e-05\n",
      "Loss: 5.86855054211e-05\n",
      "Loss: 5.81859965346e-05\n",
      "Loss: 5.76433216865e-05\n",
      "Loss: 5.72618077182e-05\n",
      "Loss: 5.6936382239e-05\n",
      "Loss: 5.66102452019e-05\n",
      "Loss: 5.63132603047e-05\n",
      "Loss: 5.55504245353e-05\n",
      "Loss: 5.41494236844e-05\n",
      "Loss: 5.13797730005e-05\n",
      "Test loss: 2.21711801414e-05\n",
      "Step:  6\n",
      "Loss: 5.01737084731e-05\n",
      "Loss: 4.96079793364e-05\n",
      "Loss: 4.69376299575e-05\n",
      "Loss: 4.56767975966e-05\n",
      "Loss: 4.28155281916e-05\n",
      "Loss: 4.14869889281e-05\n",
      "Loss: 4.07336433813e-05\n",
      "Loss: 3.94422540212e-05\n",
      "Loss: 3.88026449018e-05\n",
      "Loss: 3.7584348081e-05\n",
      "Loss: 3.77610014954e-05\n",
      "Loss: 3.66763214962e-05\n",
      "Loss: 3.64133434337e-05\n",
      "Loss: 3.59646375262e-05\n",
      "Loss: 3.58120481536e-05\n",
      "Loss: 3.56361182482e-05\n",
      "Loss: 3.53170405431e-05\n",
      "Loss: 3.49339371354e-05\n",
      "Loss: 3.45714361391e-05\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "seq = Sequence()\n",
    "seq.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "#begin to train\n",
    "for i in range(11):\n",
    "    print('Step: ', i)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq(input)\n",
    "        loss = criterion(out, target)\n",
    "        print('Loss:', loss.data.numpy()[0])\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "    # begin to predict\n",
    "    pred = seq(test_input, future = future)\n",
    "    loss = criterion(pred[:, :-future], test_target)\n",
    "    print('Test loss:', loss.data.numpy()[0])\n",
    "    y = pred.data.numpy()\n",
    "    save_plot_wave(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
